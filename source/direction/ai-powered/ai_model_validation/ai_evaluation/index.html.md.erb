---
layout: markdown_page
title: "Category Direction - AI Evaluation"
description: "An automated and scalable AI/ML model evaluation tool"
---

## On this page
{:.no_toc}

- TOC
{:toc}

## AI Evaluation 

| | |
| --- | --- |
| Stage | [AI-powered](/direction/ai-powered/) |
| Group | [AI Model Validation](/direction/ai-powered/ai_model_validation) |
| Maturity | [Planned](/direction/maturity/) |
| Content Last Reviewed | `2023-08-17` |

## Overview

AI Evaluation are critical cornerstone for successfully implementing Generative AI solutions. They ensure the reliability and the quality of code generated by AI models, mitigating the risk of introducing errors into the codebase. We want to customize AI models with rigorous evaluations to align with coding standards, industry best practices (devops principals, secure coding, test driven development, etc), and specific organizational needs, resulting in accurate and contextually relevant Code Suggestions, accurate natural language chat interactions, and generally aligned model responses. By fostering continuous improvement, we want to enhance developer productivity and contribute to building a robust and maintainable codebase, all while instilling confidence in the reliability of AI-powered processes for tasks across the software development lifecycle
Inititally as we build the foundation for AI Evlations our primary first priority is supporting Code Suggesitons quality, we’ll then expand to support Duo Chat, and after that enable evaluations for future AI-powered features.

### Goal

The AI Evaluations category will focus on assessing the performance, tuning parameters, prompt engineering techniques, and quality of algorithms for various AI models designed for code generation and completion. The models we are exploring initially for this evaluation include a subset of Google models - `code-gecko` and `text-bison` for 12 programming languages. This evaluation is crucial in developing and improving Code Suggestions, as it amplifies our understanding of how well the models are performing and identifies areas that require enhancements. We are expanding our validation to include addition use cases like Duo Chat and Explain this vulnerability features. 

## What's Next & Why

Our goal for AI evaluations on Code Suggestions is to assess what are high-quality prompts, languages, different semantics of code, the taxonomy of code completion and code generation, mapping the taxonomy, and then adding similarity metrics to historically written code. The similarity score measure how cloely the generated response aligns with the developers input and is calculated using cosine similarity. We intend to improve Code Suggestions through comprehensive and robust assessment of Generative AI models, leading to a reliable, efficient and users friendly product. As a long-term initiative, we want to evaluate various models based on Quality, Cost and Latency.

By creating a prompt library (database) and using decision science in order to make informed decisions about the models to use and prompt engineering techniques to employ.
Human Eval and the other numerous benchmarks are not good enough to decide which model would be most effective for which use case. To understand how these models work at scale, we can’t rely only on injecting manual prompts to evaluate the model. The AI Model Validation team is adding an enhanced evaluation framework of extracting and creating prompts based on the GitLab code base, on actual code written by developers, which will have a framework to understand actual agreeable developer output. The prompt/developer output will be stored in Google BigQuery and will be a prompt library. We will then evaluate the LLMs output at scale, with a framework of understanding how similar is the LLMs output to the actual agreeable developer output.

**FY24 Q3 OKR: [Increase user acceptance rate with High-Quality Suggestions](https://gitlab.com/gitlab-com/gitlab-OKRs/-/work_items/3254)**

[**Evaluation of Models in addition to Google Models**](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/prompt-library/-/issues/37) <br>
_Outcome_: We will continue to assess additional models as part of this milestone for Code Completion by adding more models from Goggle Models Garden. We are building a robust evaluation harness framework by iteratively adding the models to the dashboard, ensuring optimal scalability and readiness.

* Iteration 1: We will be adding models that are available in Google model garden
* Iteration 2: We will be adding models via Hugging Face
* Iteration 3: Deploy models in our GitLab inference service

[**Building recommendation for list of experiments for Prompt Transformation**](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/prompt-library/-/issues/27)<br>
_Outcome_: We will future investigate Prompts with lower acceptance rates in the prompt library, leading to developing a series of prompt transformation strategies outlined as hypotheses for A/B Testing. These strategies will subsequently be tested using the [A/B testing platform outlined in this scope](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/prompt-library/-/issues/10). This testing will help us to validate the effectiveness of prompt engineering in a production environment.

[**AI Evaluation for YAML**](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/prompt-library/-/issues/13)<br>
_Outcome_: We have a customer use case for a CI file where they would like to explain all the stages, also considering any other projects referenced. We have been exploring which model works best with YAML with and without prompt transformation for YAML completion as part of Code Suggestion.
Initial work to fetch the CI YAML files of GitLab projects so that they can be used for model training/tuning can be found [here](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/ai-assist/-/issues/20).
We will use this to retrain Google foundation models in order to provide code suggestions for GitLab CI YAML files as we suspect the qualiry might be low otherwise.

[**Evaluating Google Sec-Palm**](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/prompt-library/-/issues/57)<br>
_Outcome_:To explore Google Sec-Palm and its usability to explain the vulnerability feature.

Primary Decision Factors which is inspired by the [paper:] (https://arxiv.org/pdf/2203.02155.pdf)
1. Will the code be honest? (consistent with the facts),
2. Harmless (not including completion  that might offend),
3. Helpful (accomplishing the goal of the coder)? 
[Example Model Evaluations at Scale](Image link)

### Roadmap

- Model Analysis and Performance Enhancement
[Building a prompt library for model evaluation at scale](https://gitlab.com/groups/gitlab-org/modelops/applied-ml/code-suggestions/-/epics/10) 
* Analyze AI model performance across different use cases to identify which models excel in specific scenarios.
* Implement insights from the analysis to fine-tune AI models or enhance training data to improve their performance in various contexts.

- A/B Testing and Prompt Transformation
[Scope of A/B Testing Platform for Prompt Engineering Experiments](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/prompt-library/-/issues/10)
* Design A/B testing frameworks to compare the effectiveness of different prompt variations in generating correct, harmless, and helpful code.
* Implement prompt transformation strategies that adapt prompts based on use case and desired outcomes.
* Collaborate with engineering and UX teams to seamlessly integrate A/B testing and prompt transformation into the code suggestion workflow.

- Continuous Improvement and Feedback Loop
[Feedback on Iteration of Prompt Library to incorporate for next 1-3 milestones](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/prompt-library/-/issues/31#note_1565867221)
* Establish a closed feedback loop with developers to capture their insights on generated code quality, harmlessness, and helpfulness.
* Continuously refine the evaluation metrics, model fine-tuning processes, and prompt transformation strategies based on user feedback.

### Important PI milestones
We’ve established a ModelOps internal handbook PI page (internal link) which will be updated monthly as part of PI review meetings. We’re still working to actively orchestrate all our performance indicator metrics.

## Upcoming Releases:

* [16.5] - 
* [16.4] - 
* [16.3 - Increase user acceptance rate with High-Quality Code Suggestions](https://gitlab.com/gitlab-org/modelops/team-tasks/-/issues/23)
* [AdHoc manual testing to help improve the quality of Code Suggestions](https://gitlab.com/gitlab-org/gitlab/-/issues/415953)

## Analyst Research

Coming Soon

